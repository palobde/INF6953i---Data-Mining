{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usagers/gizag/.local/lib/python3.6/site-packages (0.8.1)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Wu0knw5vnGAJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from utils.data import prepare_data\n",
    "from utils.gini import eval_gini, gini_xgb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DagTeZ_ANDgk"
   },
   "source": [
    "# Laboratoire 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRO_XBj9M56w"
   },
   "source": [
    "Comme vu en classe, la classification est une part importante de la science des données. Elle consiste à apprendre à classer des échantillons et de par la suite prédire les classes de nouveaux échantillons. \n",
    "\n",
    "Durant ce laboratoire, vous allez travailler sur un concours Kaggle dont le but est de prédire la probabilité qu'un conducteur fasse une réclamation d'assurance auto durant l'année suivante. En voyant ce problème, la tâche semble être celle d'une regression et non d'un classification, cependant, avec les données disponibles, c'est effectivement une classification. En effet, les cibles ne sont pas des réels mais bien des entiers naturels puisque l'assureur peut simplement noter si le client a oui ou non fait une réclamation durant l'année d'après. Mais je vous conseille d'aller sur https://www.kaggle.com/c/porto-seguro-safe-driver-prediction pour avoir plus d'information.\n",
    "\n",
    "Vous l'aurez donc compris, il faut entraîner un prédicteur probabilistique pour avoir des probabilités durant la prédiction (ex: Regression Logistique, Bayes Naif, etc. et non SVM).\n",
    "\n",
    "Votre première tâche sera donc d'entraîner un classifieur sur l'ensemble d'entraînement et de prédire des probabilités sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCeLW6gdM56p"
   },
   "source": [
    "### Preparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiw7OqtWMh2P"
   },
   "source": [
    "Pour ce laboratoiree le but est surtout d'étudier la classification. La préparation des données est donc faite pour vous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13708,
     "status": "ok",
     "timestamp": 1526678119489,
     "user": {
      "displayName": "Alexandre Dos Santos",
      "photoUrl": "//lh3.googleusercontent.com/-x49DIhX_uz0/AAAAAAAAAAI/AAAAAAAAAKg/GJ1QqAk9EMo/s50-c-k-no/photo.jpg",
      "userId": "105252364393852282664"
     },
     "user_tz": 240
    },
    "id": "s7OoFcqBM560",
    "outputId": "cf6ba7ff-ce4f-497c-93b0-b2db0882bf8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates dropped.\n",
      "\n",
      "Under-sampling...\n",
      "Rate to undersample records with target=0: 0.34043569687437886\n",
      "Number of records with target=0 after undersampling: 195246\n",
      "\n",
      "Checking missing values...\n",
      "Variable ps_ind_02_cat has 103 records (0.05%) with missing values\n",
      "Variable ps_ind_04_cat has 51 records (0.02%) with missing values\n",
      "Variable ps_ind_05_cat has 2256 records (1.04%) with missing values\n",
      "Variable ps_reg_03 has 38580 records (17.78%) with missing values\n",
      "Variable ps_car_01_cat has 62 records (0.03%) with missing values\n",
      "Variable ps_car_02_cat has 2 records (0.00%) with missing values\n",
      "Variable ps_car_03_cat has 148367 records (68.39%) with missing values\n",
      "Variable ps_car_05_cat has 96026 records (44.26%) with missing values\n",
      "Variable ps_car_07_cat has 4431 records (2.04%) with missing values\n",
      "Variable ps_car_09_cat has 230 records (0.11%) with missing values\n",
      "Variable ps_car_11 has 1 records (0.00%) with missing values\n",
      "Variable ps_car_14 has 15726 records (7.25%) with missing values\n",
      "In total, there are 12 variables with missing values\n",
      "\n",
      "Dropping variables with more than 40% of missing values...\n",
      "Dropping ps_car_03_cat, ps_car_05_cat\n",
      "\n",
      "Replacing missing values...\n",
      "Replacing missing values of ps_reg_03 by mean\n",
      "Replacing missing values of ps_car_11 by mode\n",
      "Replacing missing values of ps_car_14 by mean\n",
      "\n",
      "Checking the cardinality of the categorical variables...\n",
      "Variable ps_ind_02_cat has 5 distinct values\n",
      "Variable ps_ind_04_cat has 3 distinct values\n",
      "Variable ps_ind_05_cat has 8 distinct values\n",
      "Variable ps_car_01_cat has 13 distinct values\n",
      "Variable ps_car_02_cat has 3 distinct values\n",
      "Variable ps_car_04_cat has 10 distinct values\n",
      "Variable ps_car_06_cat has 18 distinct values\n",
      "Variable ps_car_07_cat has 3 distinct values\n",
      "Variable ps_car_08_cat has 2 distinct values\n",
      "Variable ps_car_09_cat has 6 distinct values\n",
      "Variable ps_car_10_cat has 3 distinct values\n",
      "Variable ps_car_11_cat has 104 distinct values\n",
      "\n",
      "Dummification...\n",
      "Before dummification we have 55 variables in train\n",
      "After dummification we have 107 variables in train\n",
      "Before dummification we have 55 variables in test\n",
      "After dummification we have 107 variables in test\n",
      "\n",
      "Creating interaction variables...\n",
      "Before creating interactions we have 107 variables in train\n",
      "After creating interactions we have 162 variables in train\n",
      "Before creating interactions we have 107 variables in test\n",
      "After creating interactions we have 162 variables in test\n"
     ]
    }
   ],
   "source": [
    "data_path='./input'\n",
    "\n",
    "train = pd.read_csv(data_path+'/train.csv')\n",
    "test = pd.read_csv(data_path+'/test.csv')\n",
    "\n",
    "prep = prepare_data(train,test)\n",
    "train, targets, test = prep(True,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oz-seIskM57Y"
   },
   "source": [
    "## Classifieur classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pga1BAImNRR7"
   },
   "source": [
    "**QUESTION 1:** Entraînez un classifieur classique sur l'ensemble X_train et retournez le score Gini pour cet ensemble, puis prédisez les probabilités sur l'ensemble de test. Pour avoir les meilleurs résultats possibles (score gini), faites une recherche d'hyper-paramètres (ex:GridSearchCV) et **montrez votre travail**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9aKgkHyShyXB"
   },
   "outputs": [],
   "source": [
    "X, y = train.as_matrix()[:,1:], targets.as_matrix()\n",
    "X, y = shuffle(X,y)\n",
    "cutoff = int(len(X)*0.9)\n",
    "\n",
    "X_train_valid, y_train_valid = X[:cutoff], y[:cutoff]\n",
    "X_test, y_test = X[cutoff:], y[cutoff:]\n",
    "X_sub = test.as_matrix()[:,1:]\n",
    "\n",
    "del X, y, train, targets, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**REP**: La classifieur choisi est la regression logistique. Utilisons un GridSearch pour trouver les meilleurs parametres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__penalty': ('l1', 'l2'), 'clf__tol': (0.01, 0.0001), 'clf__C': (0.1, 1.0), 'clf__fit_intercept': (True, False), 'clf__max_iter': (300, 100), 'clf__solver': ('liblinear', 'saga')}\n",
      "Fitting 4 folds for each of 64 candidates, totalling 256 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.3min\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 16.8min\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usagers/gizag/.local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done 256 out of 256 | elapsed: 24.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1464.315s\n",
      "\n",
      "Meilleurs parametres trouves :  {'clf__C': 1.0, 'clf__fit_intercept': False, 'clf__max_iter': 300, 'clf__penalty': 'l1', 'clf__solver': 'saga', 'clf__tol': 0.01}\n",
      "Best score for validation set : 0.900\n",
      "Best score for test set : 0.273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time, clock\n",
    "\n",
    "clf = Pipeline([\n",
    "        ('clf', LogisticRegression())\n",
    "    ]);\n",
    "\n",
    "'''\n",
    "        Parameters\n",
    "        ----------\n",
    "        penalty : string, 'l1' or 'l2' (default='l2')\n",
    "            Used to specify the norm used in the penalization. The 'newton-cg',\n",
    "            'sag' and 'lbfgs' solvers support only l2 penalties.\n",
    "        dual : bool, (default=True)\n",
    "            Dual or primal formulation. Dual formulation is only implemented for\n",
    "            l2 penalty with liblinear solver. Prefer dual=False when\n",
    "            n_samples > n_features.\n",
    "        tol : float, optional (default=1e-4)\n",
    "            Tolerance for stopping criteria.\n",
    "        C : float, optional (default=1.0)\n",
    "            Inverse of regularization strength; must be a positive float.\n",
    "            Like in support vector machines, smaller values specify stronger\n",
    "            regularization.\n",
    "        fit_intercept : boolean, optional (default=True)\n",
    "            Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "            added to the decision function.\n",
    "        max_iter : int, (default=1000)\n",
    "            The maximum number of iterations to be run.\n",
    "        solver : string, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, (default: 'liblinear')\n",
    "            Algorithm to use in the optimization problem.\n",
    "            - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "                'saga' are faster for large ones.\n",
    "            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "                handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "                schemes.\n",
    "            - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "                'liblinear' and 'saga' handle L1 penalty.\n",
    "            Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
    "            features with approximately the same scale. You can\n",
    "            preprocess the data with a scaler from sklearn.preprocessing.\n",
    "'''\n",
    "\n",
    "parameters = {\n",
    "        'clf__penalty':('l1','l2'),\n",
    "        'clf__tol':(1e-2, 1e-4,),\n",
    "        'clf__C': (0.1, 1.0),\n",
    "        'clf__fit_intercept': (True, False),\n",
    "        'clf__max_iter':(300, 100,),\n",
    "        'clf__solver':('liblinear','saga')\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters,  verbose=1, iid=True, cv=4, n_jobs=-1, return_train_score=False)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "    \n",
    "grid_search.fit(X_train_valid,y_train_valid)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "    \n",
    "# Meilleurs parametres\n",
    "best_set = grid_search.best_params_\n",
    "print(\"Meilleurs parametres trouves : \", best_set)\n",
    "\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "score_train = grid_search.best_score_\n",
    "print(\"Best score for validation set : %0.3f\" % score_train)\n",
    "\n",
    "    \n",
    "probs = best_estimator.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculer le score Gini\n",
    "score_test = eval_gini(y_test,probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**REP:** Les Meilleurs parametres trouves sont :  {'C': 1.0, 'fit_intercept': False, 'max_iter': 300, 'penalty': 'l1', 'solver': 'saga', 'tol': 0.01} qui donne un score Gini test de **0.285** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for test set : 0.285\n"
     ]
    }
   ],
   "source": [
    "# Initialiser votre classifieur\n",
    "clf = LogisticRegression(C=1.0, fit_intercept=False, max_iter=300, penalty='l1', solver='saga', tol=0.01)\n",
    "\n",
    "# Entraîner le classifieur\n",
    "clf.fit(X_train_valid,y_train_valid)\n",
    "\n",
    "# Calculer les probabilités prédites sur l'ensemble de test\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculer le score Gini\n",
    "score_test = eval_gini(y_test,probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TX7rRaDBQcKI"
   },
   "source": [
    "## Méthodes d'ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmC1gteVQsBl"
   },
   "source": [
    "Les méthodes d'ensembles sont très populaires dans le machine learning pour donner de très bons résultats rapidement avec des classifieurs simples. Dans la communauté Kaggle, particulièrement, ces méthodes sont souvent utilisées et ne nécessitent pas des capacités de computation très élevées contraîrement aux réseaux de neurones.\n",
    "\n",
    "Le principe général est simple: combiner plusieurs classifieurs simples afin d'obtenir un classifieur qui généralise mieux. \n",
    "\n",
    "Pour rappel, l'erreur de généralisation peut se décomposer en deux termes: \n",
    "\n",
    "\n",
    "*   Le biais: si la famille de fonction considérée ne contient pas la fonction idéale\n",
    "*   La variance: variabilité dans la focntion trouvée due à la variabilité de l'ensemble de données\n",
    "\n",
    "La deuxième partie du laboratoire consite donc à se familiariser avec ces méthodes et de déterminer si elles aident pour cette tâche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IR06_EyTExZz"
   },
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gk3E58RBP4oS"
   },
   "source": [
    "Le boosting a principalement pour but de réduire le biais. Sont principe est le suivant:\n",
    "\n",
    "\n",
    "1.   Entraîner un classifieur simple $h_1$ sur l'ensemble de donnée initial et prédire les classes.\n",
    "2.   Déterminer les échantillons mal classés et entraîner un autre classifieur $h_{t+1}$ en augmentant l'importance de ces échantillons dans le calcul de la perte.\n",
    "3.   Combiner tous les classifieurs avec des poids différents ($\\alpha_t$) pour chaque classifieurs pour obtenir les prédictions.\n",
    "\n",
    "  Pour une prédiction binaire {-1,1}:      $H(x) = sign(\\sum_{t=1}^T \\alpha_t h_t(x))$\n",
    "\n",
    "4.   Répéter étapes 2, 3 N fois.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eed7iERyZOW4"
   },
   "source": [
    "**QUESTION 2**: Reprenez le classifieur précédent en rajoutant du boosting et comparez les résultats. \n",
    "\n",
    "HINT: Extreme Gradient Boosting semble fonctionner particulièrement bien pour ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> **REP:** Utilisons pour cela la fonction **XGBClassifier()** pour ameliorer le classifieur precedemment identifie. Le resultat Score gini est ainsi ameliore a **0.295** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "epbbNLbxG5xt"
   },
   "outputs": [],
   "source": [
    "def BoostedClassifier(X_train_, y_train_, X_valid_, y_valid_):\n",
    "    \n",
    "    from sklearn.datasets import make_hastie_10_2\n",
    "    import xgboost as xgb\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "    \n",
    "    # Initialiser le classifieur Boosté\n",
    "    clf = XGBClassifier(objective = \"reg:logistic\", n_estimators=100, learning_rate=0.1,max_depth=5, random_state=0)\n",
    "\n",
    "\n",
    "    # Entraîner le classifieur avec la métrique gini_xgb (ci-poosible)\n",
    "    clf.fit(X_train, y_train, xgb_model=None)\n",
    "    \n",
    "    # Calculer les probabilités prédites par le meilleur estimateur sur l'ensemble de validation\n",
    "    valid_probs =  clf.predict_proba(X_valid)[:,1]\n",
    "    \n",
    "    # Calculer le score Gini pour les probabilités valid_probs\n",
    "    score_valid = eval_gini(y_valid_,valid_probs)\n",
    "    print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "\n",
    "    # Calculer les probabilités prédites par le meilleur estimateur sur l'ensemble de test\n",
    "    test_probs = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    return test_probs, score_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 431438,
     "status": "ok",
     "timestamp": 1526678553983,
     "user": {
      "displayName": "Alexandre Dos Santos",
      "photoUrl": "//lh3.googleusercontent.com/-x49DIhX_uz0/AAAAAAAAAAI/AAAAAAAAAKg/GJ1QqAk9EMo/s50-c-k-no/photo.jpg",
      "userId": "105252364393852282664"
     },
     "user_tz": 240
    },
    "id": "x3T4mJx3ns2u",
    "outputId": "abc980c0-9baf-439c-d968-e7d6e04a6e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for valid set : 0.274\n",
      "Best score for test set : 0.295\n"
     ]
    }
   ],
   "source": [
    "# Séparer X_train_valid, y_train_valid en X_train, X_valid et y_train, y_valid avec un split de 0.9 (90% des données dans train et 10% dans test)\n",
    "cutoff = int(len(X_train_valid)*0.9)\n",
    "X_train, y_train = X_train_valid[:cutoff], y_train_valid[:cutoff]\n",
    "X_valid, y_valid = X_train_valid[cutoff:], y_train_valid[cutoff:]\n",
    "\n",
    "# Appeler le BoostedClassifier\n",
    "test_probs, score_valid = BoostedClassifier(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# Calculer le score Gini pour les probabilités test_probs\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fym6WDukEsRY"
   },
   "source": [
    "### Boosting + Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "is52ASGFZ5iK"
   },
   "source": [
    "La bagging est une méthode relativement simple qui a pour but de réduire la variance. Le principe est le suivant:\n",
    "1.   Séparer l'ensemble d'entraînement en N ensembles.\n",
    "2.   Entraîner N classifieurs simples sur les N ensembles.\n",
    "3.   Produire N sets de prédictions sur l'ensemble de validation avec ces prédicteurs \n",
    "4.   Faire la moyenne de ces prédictions\n",
    "\n",
    "      $H(x) = \\dfrac{1}{N} \\sum_{t=1}^N h_t(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmejXT_QbFnq"
   },
   "source": [
    "**QUESTION 3**: Utiliser la méthode du bagging en réutilisant l'algorithme boosté précédant. BONUS si vous obtenez plus de 0.3 de score Gini sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**REP**: Utilisons pour cela la fonction **BalancedBaggingClassifier()** du module imblearn qui permet en plus du Bagging, d'avoir un bagging ou les proprtions des echantillons sont balances pour etre similaires aux proportions observees dans les donnees initiales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced -- Best score for test set : 0.297\n"
     ]
    }
   ],
   "source": [
    "# Faire le Bagging de votre clasifieur bossté.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier \n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Initialiser le classifieur Boosté et avec Bagging Balance\n",
    "clf = BalancedBaggingClassifier(XGBClassifier(objective = \"reg:logistic\", n_estimators=100, learning_rate=0.1,max_depth=5, random_state=0))\n",
    "\n",
    "# Faire la moyenne des probabilitées sur l'ensemble de test\n",
    "clf.fit(X_train_valid, y_train_valid) \n",
    "test_probs = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculer le score Gini sur l'ensemble de test\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Balanced -- Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**REP**: Le Score Gini est ainsi ameliore a **0.297**! avec le bagging balance et le boosting.\n",
    "Notons que nous pourrions ameliorer davantage les resultats en faisant un **GridSearch** pour obtenir les meilleurs parametres avec les fonction de boosting et de bagging.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVN6fq58I4V4"
   },
   "source": [
    "# Pour soumettre les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SoiQHMsx68Gv"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"submission.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "    csv_writer.writerow([\"id\",\"target\"])\n",
    "    for i, prob in enumerate(test_probs):\n",
    "        csv_writer.writerow([i,prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIYDhWeiPCQ1"
   },
   "source": [
    "Excecutez la commande:\n",
    "\n",
    "`kaggle competitions submit -c porto-seguro-safe-driver-prediction -f submission.csv -m \"Message\"`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Laboratoire_2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membres :\n",
    "\n",
    "## Foromo Daniel Soromou  1759116\n",
    "## Gilles Eric Zagre   1146014\n",
    "## Roger Lobe 1679117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe tous les modules nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cellule qui installe les modules necessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usagers/gizag/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usagers/gizag/.local/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scipy in /usagers/gizag/.local/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from scipy) (1.14.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gensim in /usagers/gizag/.local/lib/python3.6/site-packages (3.4.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usagers/gizag/.local/lib/python3.6/site-packages (from gensim) (1.5.7)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usagers/gizag/.local/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: boto>=2.32 in /usagers/gizag/.local/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: bz2file in /usagers/gizag/.local/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: requests in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: boto3 in /usagers/gizag/.local/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (1.7.15)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usagers/gizag/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usagers/gizag/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.15 in /usagers/gizag/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.24)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.15->boto3->smart-open>=1.2.1->gensim) (2.7.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.15->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usagers/gizag/.local/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: smart_open in /usagers/gizag/.local/lib/python3.6/site-packages (1.5.7)\n",
      "Requirement already satisfied: boto>=2.32 in /usagers/gizag/.local/lib/python3.6/site-packages (from smart_open) (2.48.0)\n",
      "Requirement already satisfied: bz2file in /usagers/gizag/.local/lib/python3.6/site-packages (from smart_open) (0.98)\n",
      "Requirement already satisfied: requests in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from smart_open) (2.18.4)\n",
      "Requirement already satisfied: boto3 in /usagers/gizag/.local/lib/python3.6/site-packages (from smart_open) (1.7.15)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart_open) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart_open) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart_open) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from requests->smart_open) (1.22)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.15 in /usagers/gizag/.local/lib/python3.6/site-packages (from boto3->smart_open) (1.10.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usagers/gizag/.local/lib/python3.6/site-packages (from boto3->smart_open) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usagers/gizag/.local/lib/python3.6/site-packages (from boto3->smart_open) (0.1.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.15->boto3->smart_open) (2.7.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.15->boto3->smart_open) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.11.0,>=1.10.15->boto3->smart_open) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: progressbar2 in /usagers/gizag/.local/lib/python3.6/site-packages (3.37.1)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usagers/gizag/.local/lib/python3.6/site-packages (from progressbar2) (2.3.0)\n",
      "Requirement already satisfied: six in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from progressbar2) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: testfixtures in /usagers/gizag/.local/lib/python3.6/site-packages (6.0.2)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: botocore in /usagers/gizag/.local/lib/python3.6/site-packages (1.10.24)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore) (0.9.3)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore) (2.7.0)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /usagers/gizag/.local/lib/python3.6/site-packages (from botocore) (0.14)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /home/local/python3.6-GIGL/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user sklearn\n",
    "!pip install --user scipy\n",
    "!pip install --user gensim\n",
    "!pip install --user nltk\n",
    "!pip install --user smart_open\n",
    "!pip install --user progressbar2\n",
    "!pip install --user testfixtures\n",
    "!pip install --user botocore --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IMPORTANT__: Redemarrez le notebook apres avoir execute cette cellule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import botocore\n",
    "\n",
    "\n",
    "import locale, random, glob, urllib, tarfile, sys, os, datetime, multiprocessing\n",
    "import numpy as np\n",
    "from time import time, clock\n",
    "from pprint import pprint;\n",
    "from math import floor;\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "import gensim.models.doc2vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "from smart_open import smart_open\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import progressbar\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette cellule télécharge le dataset ImDB Sentiment de Stanford. Remplacez le root_path par le path où vous voulez télécharger les données. Un prétraitement très rapide a été effectué pour remplacer les balises <*br /> par des espaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n==============FIN DE VOTRE CODE=====================\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "==============ÉCRIVEZ DU CODE ICI====================\n",
    "'''\n",
    "import re\n",
    "def clean_balise(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html )\n",
    "    cleantext = re.sub(' +', \" \", cleantext)\n",
    "    return cleantext\n",
    "\n",
    "def replace_ponctuation(raw_text):\n",
    "    raw_text = raw_text.replace(\"...\", \"&&&\")\n",
    "    raw_text = raw_text.replace(\".\", \" . \").replace(\",\", \" , \").replace(\";\", \" ; \").replace(\":\", \" : \").replace(\"!\", \" ! \").replace(\"?\", \" ? \").replace(\"&&&\", \" &&& \").replace(\"(\", \" ( \").replace(\")\", \" ) \").replace('\"', ' \" ')\n",
    "                        \n",
    "    return re.sub(' +', \" \", raw_text).replace(\"&&&\", \"...\")\n",
    "\n",
    "# Enlevez les balises html et séparez les ponctuations du texte\n",
    "def normalize_text(text):\n",
    "    # Remplacez les balises par des espaces ici\n",
    "    norm_text = replace_ponctuation(clean_balise(text).strip())\n",
    "    print(norm_text[:100])\n",
    "    return norm_text\n",
    "'''\n",
    "==============FIN DE VOTRE CODE=====================\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IMDB archive...\n",
      "Extracting IMDB archive...\n",
      "Cleaning up dataset...\n",
      "Total running time:  15.060528999999999\n"
     ]
    }
   ],
   "source": [
    "root_path = './'\n",
    "\n",
    "dirname = 'aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "dirpath = os.path.join(root_path,'imdb/')\n",
    "filepath = os.path.join('./imdb/', filename)\n",
    "urlpath = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "if not os.path.exists('./imdb/'):\n",
    "    os.makedirs('./imdb/')\n",
    "\n",
    "start = clock()\n",
    "\n",
    "if not os.path.isfile(dirname+'/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filepath):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            urllib.request.urlretrieve(urlpath, filepath);\n",
    "        print(\"Extracting IMDB archive...\")\n",
    "        tar = tarfile.open(filepath, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "    # Concatenate and normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    alldata = u''\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        for txt in txt_files:\n",
    "            with smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "                temp += t_clean\n",
    "            temp += \"\\n\"\n",
    "        temp_norm = normalize_text(temp)\n",
    "        \n",
    "        \n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp_norm.encode(\"utf-8\"))\n",
    "        alldata += temp_norm\n",
    "\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"{1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "else:\n",
    "    print('The data has already been downloaded')\n",
    "    \n",
    "    # Concatenate and normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    alldata = u''\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        for txt in txt_files:\n",
    "            with smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "                temp += t_clean\n",
    "            temp += \"\\n\"\n",
    "        temp_norm = normalize_text(temp)\n",
    "        \n",
    "        \n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp_norm.encode(\"utf-8\"))\n",
    "        alldata += temp_norm\n",
    "\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"{1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "    \n",
    "            \n",
    "end = clock()\n",
    "print (\"Total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouverture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Taille de alldata:  100000\n",
      "***Taille de X_unsup:  50000\n",
      "***Taille de X_train:  25000\n",
      "***Taille de X_test:  25000\n"
     ]
    }
   ],
   "source": [
    "with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'rb') as alldata:\n",
    "    alldata = alldata.readlines()\n",
    "    X_unsup = []\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        if line_no // 25000 < 1:\n",
    "            X_train.append(line.decode(\"utf-8\"))\n",
    "        elif line_no // 25000 < 2:\n",
    "            X_test.append(line.decode(\"utf-8\"))   \n",
    "        else:\n",
    "            X_unsup.append(line.decode(\"utf-8\"))\n",
    "        \n",
    "y_train = [1.0]*12500 + [0.0]*12500\n",
    "y_test = [1.0]*12500 + [0.0]*12500\n",
    "\n",
    "print('***Taille de alldata: ',len(alldata))\n",
    "print('***Taille de X_unsup: ',len(X_unsup))\n",
    "print('***Taille de X_train: ',len(X_train))\n",
    "print('***Taille de X_test: ',len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le prétraitement des données est une étape importante, en particulier lorsque vous travaillez avec du texte. Dans la science des données, c'est très souvent la tâche qui demande le plus de temps. Comme vous l'aurez compris, le but du laboratoire est d'étudier l'influence du prétraitement sur la performance. \n",
    "\n",
    "La fonction ci-dessous est ce que l'on appelle un \"tokenizer\", son but est de créer une liste de \"tokens\" à partir d'une phrase (single string). Le tâche peut sembler triviale à première vue, mais l'on peut vite s'apercevoir qu'elle ne l'est pas tant (ex: \"It's the student's notebook.\" -> ['It', 'is', 'the', 'sutdent, 'notebook', '.'] ou [\"It's\", \"the\", \"student's\", \"notebook.\"] ?).\n",
    "\n",
    "En plus du tokenizer, il est possible de rajouter un stemmer qui extrait la racine des mots (ex: \"generously\" -> \"generous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n==============FIN DE VOTRE CODE=====================\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Parameters:\n",
    "    use_nltk_tok(bool): either to use nltk word tokenizer (True) or basic space split (False)\n",
    "    use_stemmer(bool): either to use the nltk stemmer (takes the root of each word)\n",
    "'''\n",
    "'''\n",
    "==============ÉCRIVEZ DU CODE ICI====================\n",
    "'''\n",
    "class custom_tokenizer():\n",
    "    def __init__(self, use_nltk_tok=False, use_stemmer=False):\n",
    "        self.use_nltk_tok = use_nltk_tok\n",
    "        self.use_stemmer = use_stemmer\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        \n",
    "    def __call__(self, string):\n",
    "        # Utilisez le tokenizer de nltk\n",
    "        if self.use_nltk_tok:\n",
    "            tokens = word_tokenize(string)\n",
    "        # Séparez simplement les mots par les espaces\n",
    "        else:\n",
    "            tokens = string.split(' ')\n",
    "        # \n",
    "        if self.use_stemmer:\n",
    "            words = [self.stemmer.stem(token) for token in tokens] \n",
    "        else:\n",
    "            words = tokens\n",
    "        return words\n",
    "'''\n",
    "==============FIN DE VOTRE CODE=====================\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte est ce que l'on appelle une donnée non structurée, c'est à dire qu'il ne possède pas d'attribut (\"feature\") fixe et mesurable. Or pour la majorité des algorithmes d'appentissage il est nécessaire d'avoir des vecteurs de même dimension en entrée. Ci-dessous se trouve donc l'extracteur de features, qui transforme toutes les phrases d'un corpus en vecteurs de même dimension. \n",
    "\n",
    "L'extraction de feature pour le texte est encore un sujet de recherche, mais quelques méthodes existantes sont déjà relativement efficaces. Les méthodes étudiées durant ce laboratoire sont l'approche Bag-of-Word et Doc2Vec.\n",
    "\n",
    "__QUESTION 1__:\n",
    "Faites une petite recherche de littérature sur ces méthodes et expliquez-les avec vos mots (5-10 lignes par méthode). Une bonne piste de départ sont les articles https://arxiv.org/pdf/1301.3781.pdf et https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voir rapport PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTION 2__: D'après vous, quels sont les avantages de Doc2Vec face à BoW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voir rapport PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTION 3__: Décrivez rapidement qu'est qu'un bigram. En quoi peut-il améliorer les performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voir rapport PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "    X_unsup(list): list of sentences (str) with no label\n",
    "    X_train(list): list of sentences (str) of the training set\n",
    "    X_test(list): list of sentences (str) of the test set\n",
    "    use_Doc2Vec(bool): either to use Doc2Vec (True) or Bag-of-words (False)\n",
    "    use_bigram(bool): either to create bigrams with the most common sequence of two words (ex: ['New', 'York'] -> ['New_York'])\n",
    "    use_nltk_tok(bool): either to use nltk word tokenizer (True) or basic space split (False)\n",
    "    use_stemmer(bool): either to use the nltk stemmer (takes the root of each word)\n",
    "'''\n",
    "\n",
    "def prepare_data(X_unsup, X_train, X_test, use_Doc2Vec=False, use_bigram=False, use_nltk_tok=False, use_stemmer=False):\n",
    "    \n",
    "    X =  X_train + X_test + X_unsup\n",
    "    \n",
    "    kwargs = {\"use_nltk_tok\":use_nltk_tok, \"use_stemmer\":use_stemmer}\n",
    "    tokenizer = custom_tokenizer(**kwargs)\n",
    "    \n",
    "    if use_Doc2Vec:\n",
    "        \n",
    "        SentimentDocument = namedtuple('SentimentDocument', 'words tags')\n",
    "        \n",
    "        print('Creating the docs for Doc2Vec...')\n",
    "        allreviews = []\n",
    "        for review in X:\n",
    "            words = tokenizer(review)\n",
    "            allreviews.append(words)\n",
    "        \n",
    "        if use_bigram:\n",
    "            print('Training bigram transormer...')\n",
    "            phrases = Phrases(allreviews)\n",
    "            bigram = Phraser(phrases)\n",
    "            # Apellez bigram pour chaque review de la liste allreviews. \n",
    "            # Il va permettre de remplacer les couples de mots qui reviennent souvent par un seul token\n",
    "            allreviews = [bigram[review] for review in allreviews]\n",
    "        \n",
    "        alldocs = []\n",
    "        for line_no, words in enumerate(allreviews):\n",
    "            tags = [line_no] \n",
    "            alldocs.append(SentimentDocument(words, tags))\n",
    "        \n",
    "        # Ici il s'agît de récupérer les documents qui serviront pour le test\n",
    "        test_docs = alldocs[len(X_train):-len(X_unsup)]\n",
    "        print('*** Taille de testdocs: ',len(test_docs))\n",
    "        # Ici il faut récupérer les documents qui serviront pour l'entraîenement du model\n",
    "        train_docs = alldocs[:len(X_train)] + alldocs[-len(X_unsup):]\n",
    "        \n",
    "        print('Creating Doc2Vec models...')\n",
    "        simple_models = [\n",
    "            # PV-DBOW \n",
    "            Doc2Vec(dm=0, vector_size=vec_size, negative=5, hs=0, min_count=2, workers=cores),\n",
    "            # PV-DM w/ average\n",
    "            Doc2Vec(dm=1, dm_mean=1, vector_size=vec_size, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "        ]\n",
    "\n",
    "        print('Creating a vocabulary...')\n",
    "        simple_models[0].build_vocab(train_docs)\n",
    "        for model in simple_models[1:]:\n",
    "            model.reset_from(simple_models[0])\n",
    "\n",
    "        models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "        models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "\n",
    "        alpha, min_alpha, passes = (0.025, 0.001, 20) \n",
    "        alpha_delta = (alpha - min_alpha) / passes\n",
    "        print('Vectorizing the data... %s' % datetime.datetime.now())\n",
    "        \n",
    "        print('Starting training...')\n",
    "        for epoch in range(passes):\n",
    "            random.shuffle(train_docs)  # Shuffling gets best results\n",
    "\n",
    "            for name, train_model in models_by_name.items():\n",
    "                # Train\n",
    "                train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "                train_model.train(train_docs, total_examples=len(train_docs), epochs=1)\n",
    "\n",
    "            print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "            alpha -= alpha_delta\n",
    "\n",
    "        print('Vectorization done %s' % str(datetime.datetime.now()))\n",
    "        \n",
    "        X_train_vect = np.zeros((len(X_train), vec_size*2))\n",
    "        X_test_vect = np.zeros((len(X_test), vec_size*2))\n",
    "        \n",
    "        # Maintenant remplacez les reviews par leur vecteurs \n",
    "        # !!ATTENTION!! Dans le diminuer le biais dans l'expérience, \n",
    "        # il faut inferrer les vecteurs pour l'ensemble de test\n",
    "        print('Inferring vectors...')\n",
    "        for i in range(len(X_train)):\n",
    "            # Simplement le .docvec du document\n",
    "            X_train_vect[i,:] = models_by_name['dbow+dmm'].docvecs[i]\n",
    "            # ici il faut infer_vector depuis le modèle entraîné précédemment\n",
    "            X_test_vect[i,:] = models_by_name['dbow+dmm'].infer_vector(alldocs[i].words)\n",
    "    \n",
    "    else:\n",
    "        if use_bigram:\n",
    "            ngram_range = (1,2)\n",
    "        else:\n",
    "            ngram_range = (1,1)\n",
    "\n",
    "        print('Creating a vocabulary...')\n",
    "\n",
    "        CV = CountVectorizer(analyzer=\"word\", tokenizer=tokenizer, ngram_range=ngram_range, max_features=vocab_size)\n",
    "        \n",
    "        CV.fit_transform(X_train + X_unsup)\n",
    "\n",
    "        vocab = CV.vocabulary_\n",
    "        stop_words = CV.stop_words_\n",
    "\n",
    "        print('Vectorizing the data... %s' % datetime.datetime.now())\n",
    "\n",
    "        CV = CountVectorizer(analyzer=\"word\", ngram_range=ngram_range, tokenizer=custom_tokenizer(**kwargs), preprocessor=lambda text: text.replace(\"<br />\", \" \"), vocabulary=vocab, stop_words=stop_words)\n",
    "\n",
    "        X_train_vect = CV.fit_transform(X_train)\n",
    "        X_test_vect = CV.fit_transform(X_test)\n",
    "\n",
    "        print('Vectorization done %s' % str(datetime.datetime.now()))\n",
    "        \n",
    "    return X_train_vect, X_test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, se trouve la ligne à executer pour obtenir les vecteurs à partir de chaque document du corpus. Changez kwargs pour avoir différents prétraitements et feature extracteur.\n",
    "\n",
    "NB: Certains algorithmes sont plus long que d'autres (si tout est activé), soyez patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usagers/gizag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 15:53:00.270993\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 15:59:51.376187\n",
      "Inferring vectors...\n",
      "Number of features 20\n",
      "Number of features 20\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "kwargs = {\n",
    "    \"use_Doc2Vec\":True, \n",
    "    \"use_bigram\":False, \n",
    "    \"use_nltk_tok\":False, \n",
    "    \"use_stemmer\":False\n",
    "}\n",
    "\n",
    "vocab_size = None # For the BoW approach. No bound when set to None\n",
    "vec_size = 10 # For the Doc2Vec approach.\n",
    "\n",
    "X_train_vec, X_test_vec = prepare_data(X_unsup, X_train, X_test, **kwargs)\n",
    "\n",
    "n_features = X_train_vec.shape[1]\n",
    "\n",
    "print('Number of features', n_features)\n",
    "print('Number of features', X_test_vec.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que les features sont extraits, il faut entraîner un classifieur. Une regression logistique a été choisi pour cette tâche, mais si vous êtes curieux, vous pouvez en essayer d'autres (Les classifieurs simples les plus populaires pour la classification de texte sont: Bayes Naif, SVM linéaire mais vous pouvez choisir autre chose).\n",
    "\n",
    "N.B.: Ici, un Grid Search avec Cross-Validation a été mit en place pour vous familiariser avec les méthodes d'ensemble qui seront le sujet du laboratoire 2. Il n'est pas nécessaire de le comprendre pour le moment, mais si vous êtes curieux, vous pouvez vous renseigner sur le sujet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('clf', LogisticRegression())\n",
    "    ]);\n",
    "\n",
    "    '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        penalty : string, 'l1' or 'l2' (default='l2')\n",
    "            Used to specify the norm used in the penalization. The 'newton-cg',\n",
    "            'sag' and 'lbfgs' solvers support only l2 penalties.\n",
    "        dual : bool, (default=True)\n",
    "            Dual or primal formulation. Dual formulation is only implemented for\n",
    "            l2 penalty with liblinear solver. Prefer dual=False when\n",
    "            n_samples > n_features.\n",
    "        tol : float, optional (default=1e-4)\n",
    "            Tolerance for stopping criteria.\n",
    "        C : float, optional (default=1.0)\n",
    "            Inverse of regularization strength; must be a positive float.\n",
    "            Like in support vector machines, smaller values specify stronger\n",
    "            regularization.\n",
    "        fit_intercept : boolean, optional (default=True)\n",
    "            Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "            added to the decision function.\n",
    "        max_iter : int, (default=1000)\n",
    "            The maximum number of iterations to be run.\n",
    "        solver : string, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, (default: 'liblinear')\n",
    "            Algorithm to use in the optimization problem.\n",
    "            - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "                'saga' are faster for large ones.\n",
    "            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "                handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "                schemes.\n",
    "            - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "                'liblinear' and 'saga' handle L1 penalty.\n",
    "            Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
    "            features with approximately the same scale. You can\n",
    "            preprocess the data with a scaler from sklearn.preprocessing.\n",
    "    '''\n",
    "\n",
    "    parameters = {\n",
    "#         'clf__penalty': ('l2', 'l1'),\n",
    "#         'clf__dual':(True, False),\n",
    "#         'clf__tol':(1e-2, ),\n",
    "#         'clf__C': (0.1, ),\n",
    "#         'clf__fit_intercept': (True, False).\n",
    "        'clf__max_iter':(300,),\n",
    "#         'solver':('liblinear',)\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(clf, parameters,  verbose=1, iid=True, cv=4, n_jobs=-1, return_train_score=False)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train_vec, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    score_train = grid_search.best_score_\n",
    "    print(\"Best score for validation set : %0.3f\" % score_train)\n",
    "    \n",
    "    tic= time()\n",
    "    predictions = best_estimator.predict(X_test_vec);\n",
    "    print(\"Prediction done in %0.3fs\" % (time() - tic))\n",
    "    score_test = metrics.accuracy_score(y_test, predictions)\n",
    "    print(\"Best score for test set : %0.3f\" % score_test)\n",
    "    print(metrics.classification_report(y_test, predictions, digits=4))\n",
    "    \n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.180s\n",
      "\n",
      "Best score for validation set : 0.831\n",
      "Prediction done in 0.001s\n",
      "Best score for test set : 0.501\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.5098    0.0519    0.0942     12500\n",
      "        1.0     0.5005    0.9501    0.6556     12500\n",
      "\n",
      "avg / total     0.5052    0.5010    0.3749     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_score, test_score = Classifier(X_train_vec, y_train, X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTION 4__: Pour l'approche BoW, tracez sur un même graphique les précisions obtenues sur l'ensemble d'entraînement et de test, pour chaque option de prétraitement, en faisant varier la taille du vocabulaire (au moins :1000, 10000, 100000, None). En résumé, un total de 6 courbes avec la précision en y, et la taille du vocabulaire en x). __Commentez les résultats__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voir rapport PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option BoW:\n",
    "\n",
    "# Construction de la liste des options d'execution\n",
    "lok = []\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":False, \"use_nltk_tok\":False, \"use_stemmer\":False })\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":False, \"use_nltk_tok\":False, \"use_stemmer\":True })\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":False, \"use_nltk_tok\":True, \"use_stemmer\":False })\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":False, \"use_nltk_tok\":True, \"use_stemmer\":True })\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":True, \"use_nltk_tok\":False, \"use_stemmer\":False })\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":True, \"use_nltk_tok\":False,  \"use_stemmer\":True})\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":True, \"use_nltk_tok\":True, \"use_stemmer\":False })\n",
    "lok.append({\"use_Doc2Vec\":False, \"use_bigram\":True, \"use_nltk_tok\":True, \"use_stemmer\":True })\n",
    "\n",
    "# Pour chaque set de configuration (options)\n",
    "for args in lok:\n",
    "    list_of_train_score = []\n",
    "    list_of_test_score = []\n",
    "    n_feature = -1\n",
    "    \n",
    "    # Pour differentes taille de vocabulaire (None correspond a non specifie)\n",
    "    for k in [1000, 10000, 100000, None]:\n",
    "        vocab_size = k\n",
    "        \n",
    "        # Pretraitement des donnees et extraction de features\n",
    "        X_train_vec, X_test_vec = prepare_data(X_unsup, X_train, X_test, **args)\n",
    "        if k == None : \n",
    "            n_feature = X_train_vec.shape[1]\n",
    "            \n",
    "        # Classification\n",
    "        train_score, test_score = Classifier(X_train_vec, y_train, X_test_vec, y_test)\n",
    "        \n",
    "        # Memorisation des donnees\n",
    "        list_of_train_score.append(train_score)\n",
    "        list_of_test_score.append(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTION 5__: Pour l'approche Doc2Vec, tracez sur un même graphique les précisions obtenues sur l'ensemble d'entraînement et de test, pour chaque option de prétraitement, en faisant varier le nombre de features (vec_size) (au moins :4, 8, 64, 256). En résumé, un total de 6 courbes avec la précision en y, et la taille du vecteur en x). __Commentez les résultats__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voir rapport PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usagers/gizag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usagers/gizag/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "***OPTION :  1\n",
      "   ***VecSize = 4\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 21:53:35.369903\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 22:01:02.223126\n",
      "Inferring vectors...\n",
      "Number of features 8\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.497s\n",
      "\n",
      "Best score for validation set : 0.691\n",
      "Prediction done in 0.010s\n",
      "Best score for test set : 0.684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.6583    0.7648    0.7076     12500\n",
      "        1.0     0.7194    0.6030    0.6561     12500\n",
      "\n",
      "avg / total     0.6889    0.6839    0.6818     25000\n",
      "\n",
      "      *** Results :  1 ,  4 ,  0.69148 ,  0.68392\n",
      "   ***VecSize = 8\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 22:02:19.093887\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 22:10:27.330566\n",
      "Inferring vectors...\n",
      "Number of features 16\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.580s\n",
      "\n",
      "Best score for validation set : 0.809\n",
      "Prediction done in 0.013s\n",
      "Best score for test set : 0.814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.7970    0.8418    0.8188     12500\n",
      "        1.0     0.8324    0.7856    0.8083     12500\n",
      "\n",
      "avg / total     0.8147    0.8137    0.8136     25000\n",
      "\n",
      "      *** Results :  1 ,  8 ,  0.8092 ,  0.81372\n",
      "   ***VecSize = 64\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 22:11:55.417623\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 22:19:36.654269\n",
      "Inferring vectors...\n",
      "Number of features 128\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.988s\n",
      "\n",
      "Best score for validation set : 0.888\n",
      "Prediction done in 0.012s\n",
      "Best score for test set : 0.888\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.8782    0.9005    0.8892     12500\n",
      "        1.0     0.8979    0.8751    0.8864     12500\n",
      "\n",
      "avg / total     0.8880    0.8878    0.8878     25000\n",
      "\n",
      "      *** Results :  1 ,  64 ,  0.88756 ,  0.8878\n",
      "   ***VecSize = 256\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 22:21:23.921046\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 22:33:00.264575\n",
      "Inferring vectors...\n",
      "Number of features 512\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.948s\n",
      "\n",
      "Best score for validation set : 0.885\n",
      "Prediction done in 0.016s\n",
      "Best score for test set : 0.875\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.8913    0.8550    0.8728     12500\n",
      "        1.0     0.8607    0.8958    0.8779     12500\n",
      "\n",
      "avg / total     0.8760    0.8754    0.8753     25000\n",
      "\n",
      "      *** Results :  1 ,  256 ,  0.88524 ,  0.8754\n",
      "***OPTION :  2\n",
      "   ***VecSize = 4\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 22:37:16.320786\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 22:45:36.562189\n",
      "Inferring vectors...\n",
      "Number of features 8\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.640s\n",
      "\n",
      "Best score for validation set : 0.706\n",
      "Prediction done in 0.010s\n",
      "Best score for test set : 0.691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.6650    0.7693    0.7133     12500\n",
      "        1.0     0.7263    0.6124    0.6645     12500\n",
      "\n",
      "avg / total     0.6957    0.6908    0.6889     25000\n",
      "\n",
      "      *** Results :  2 ,  4 ,  0.70572 ,  0.69084\n",
      "   ***VecSize = 8\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 22:49:25.821868\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 22:57:30.057589\n",
      "Inferring vectors...\n",
      "Number of features 16\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.063s\n",
      "\n",
      "Best score for validation set : 0.796\n",
      "Prediction done in 0.003s\n",
      "Best score for test set : 0.787\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.7754    0.8085    0.7916     12500\n",
      "        1.0     0.7999    0.7658    0.7825     12500\n",
      "\n",
      "avg / total     0.7877    0.7872    0.7871     25000\n",
      "\n",
      "      *** Results :  2 ,  8 ,  0.7958 ,  0.78716\n",
      "   ***VecSize = 64\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 23:02:27.176803\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 23:11:08.590588\n",
      "Inferring vectors...\n",
      "Number of features 128\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.671s\n",
      "\n",
      "Best score for validation set : 0.884\n",
      "Prediction done in 0.009s\n",
      "Best score for test set : 0.884\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.8709    0.9027    0.8865     12500\n",
      "        1.0     0.8990    0.8662    0.8823     12500\n",
      "\n",
      "avg / total     0.8850    0.8845    0.8844     25000\n",
      "\n",
      "      *** Results :  2 ,  64 ,  0.88384 ,  0.88448\n",
      "   ***VecSize = 256\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 23:14:22.920417\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 23:23:34.967352\n",
      "Inferring vectors...\n",
      "Number of features 512\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.560s\n",
      "\n",
      "Best score for validation set : 0.881\n",
      "Prediction done in 0.013s\n",
      "Best score for test set : 0.868\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.8866    0.8437    0.8646     12500\n",
      "        1.0     0.8509    0.8921    0.8710     12500\n",
      "\n",
      "avg / total     0.8687    0.8679    0.8678     25000\n",
      "\n",
      "      *** Results :  2 ,  256 ,  0.88148 ,  0.86788\n",
      "***OPTION :  3\n",
      "   ***VecSize = 4\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 23:28:06.552702\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 23:34:52.473144\n",
      "Inferring vectors...\n",
      "Number of features 8\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.510s\n",
      "\n",
      "Best score for validation set : 0.717\n",
      "Prediction done in 0.002s\n",
      "Best score for test set : 0.706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.6880    0.7538    0.7194     12500\n",
      "        1.0     0.7278    0.6582    0.6913     12500\n",
      "\n",
      "avg / total     0.7079    0.7060    0.7053     25000\n",
      "\n",
      "      *** Results :  3 ,  4 ,  0.71692 ,  0.706\n",
      "   ***VecSize = 8\n",
      "Creating the docs for Doc2Vec...\n",
      "*** Taille de testdocs:  25000\n",
      "Creating Doc2Vec models...\n",
      "Creating a vocabulary...\n",
      "Vectorizing the data... 2018-05-21 23:38:32.766700\n",
      "Starting training...\n",
      "Completed pass 1 at alpha 0.025000\n",
      "Completed pass 2 at alpha 0.023800\n",
      "Completed pass 3 at alpha 0.022600\n",
      "Completed pass 4 at alpha 0.021400\n",
      "Completed pass 5 at alpha 0.020200\n",
      "Completed pass 6 at alpha 0.019000\n",
      "Completed pass 7 at alpha 0.017800\n",
      "Completed pass 8 at alpha 0.016600\n",
      "Completed pass 9 at alpha 0.015400\n",
      "Completed pass 10 at alpha 0.014200\n",
      "Completed pass 11 at alpha 0.013000\n",
      "Completed pass 12 at alpha 0.011800\n",
      "Completed pass 13 at alpha 0.010600\n",
      "Completed pass 14 at alpha 0.009400\n",
      "Completed pass 15 at alpha 0.008200\n",
      "Completed pass 16 at alpha 0.007000\n",
      "Completed pass 17 at alpha 0.005800\n",
      "Completed pass 18 at alpha 0.004600\n",
      "Completed pass 19 at alpha 0.003400\n",
      "Completed pass 20 at alpha 0.002200\n",
      "Vectorization done 2018-05-21 23:45:00.051351\n",
      "Inferring vectors...\n",
      "Number of features 16\n",
      "Performing grid search...\n",
      "parameters:\n",
      "{'clf__max_iter': (300,)}\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.584s\n",
      "\n",
      "Best score for validation set : 0.789\n",
      "Prediction done in 0.007s\n",
      "Best score for test set : 0.779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.7612    0.8138    0.7867     12500\n",
      "        1.0     0.8000    0.7447    0.7714     12500\n",
      "\n",
      "avg / total     0.7806    0.7793    0.7790     25000\n",
      "\n",
      "      *** Results :  3 ,  8 ,  0.78904 ,  0.77928\n",
      "   ***VecSize = 64\n",
      "Creating the docs for Doc2Vec...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1c3ed9066425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Pretraitement des donnees et extraction des features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-74e7ce5bc478>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(X_unsup, X_train, X_test, use_Doc2Vec, use_bigram, use_nltk_tok, use_stemmer)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mallreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mallreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-4c0689f8f574>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Utilisez le tokenizer de nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_nltk_tok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Séparez simplement les mots par les espaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Option Doc2Vec:\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Liste pour faire varier le nombre de features (vec_size): \n",
    "vec_size_list = [4, 8, 64, 256]\n",
    "\n",
    "# Liste pour faire varier les options de pretraitement\n",
    "kwargsDoc2Vec_1 = {\"use_Doc2Vec\":True, \"use_bigram\":False, \"use_nltk_tok\":False, \"use_stemmer\":False}\n",
    "kwargsDoc2Vec_2 = {\"use_Doc2Vec\":True, \"use_bigram\":False, \"use_nltk_tok\":False, \"use_stemmer\":True}\n",
    "kwargsDoc2Vec_3 = {\"use_Doc2Vec\":True, \"use_bigram\":False, \"use_nltk_tok\":True, \"use_stemmer\":False}\n",
    "kwargsDoc2Vec_4 = {\"use_Doc2Vec\":True, \"use_bigram\":False, \"use_nltk_tok\":True, \"use_stemmer\":True}\n",
    "kwargsDoc2Vec_5 = {\"use_Doc2Vec\":True, \"use_bigram\":True, \"use_nltk_tok\":False, \"use_stemmer\":False}\n",
    "kwargsDoc2Vec_6 = {\"use_Doc2Vec\":True, \"use_bigram\":True, \"use_nltk_tok\":False, \"use_stemmer\":True}\n",
    "kwargsDoc2Vec_7 = {\"use_Doc2Vec\":True, \"use_bigram\":True, \"use_nltk_tok\":True, \"use_stemmer\":False}\n",
    "kwargsDoc2Vec_8 = {\"use_Doc2Vec\":True, \"use_bigram\":True, \"use_nltk_tok\":True, \"use_stemmer\":True}\n",
    "\n",
    "kwargs_List = [kwargsDoc2Vec_1, kwargsDoc2Vec_2, kwargsDoc2Vec_3, kwargsDoc2Vec_4, kwargsDoc2Vec_5, kwargsDoc2Vec_6, kwargsDoc2Vec_7, kwargsDoc2Vec_8]\n",
    "\n",
    "# Pour chaque option de pretraitement\n",
    "RESULTS_TAB = []\n",
    "i=1\n",
    "for kwargs in kwargs_List:\n",
    "    # Pour chaque taille de vecteur\n",
    "    print('***OPTION : ',i)\n",
    "    i+=1\n",
    "    for vec_size in vec_size_list: \n",
    "        print('   ***VecSize =',vec_size)\n",
    "        \n",
    "        Resultats=[]\n",
    "        \n",
    "        # Pretraitement des donnees et extraction des features\n",
    "        X_train_vec, X_test_vec = prepare_data(X_unsup, X_train, X_test, **kwargs)\n",
    "        n_features = X_train_vec.shape[1]\n",
    "        print('Number of features', n_features)\n",
    "        \n",
    "        # Classification\n",
    "        train_score, test_score = Classifier(X_train_vec, y_train, X_test_vec, y_test)\n",
    "        \n",
    "        # Rajouter les resultats pour cette taille de vecteur\n",
    "        Resultats.append([vec_size, train_score, test_score])\n",
    "        print('      *** Results : ', i-1,', ',vec_size,', ',train_score,', ' ,test_score)\n",
    "    \n",
    "    # Rajouter les Resultats pour ce set d'option    \n",
    "    RESULTS_TAB.append(Resultats)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BONUS__: Vous pouvez aussi faire varier les autres paramètres du Doc2Vec (dm, negative, hs, min_count, window) et étudier leurs impacts sur les résultats à l'aide de courbes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
